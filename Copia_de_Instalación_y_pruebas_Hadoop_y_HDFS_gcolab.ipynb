{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alebernabe5/BigDataAplicada/blob/main/Copia_de_Instalaci%C3%B3n_y_pruebas_Hadoop_y_HDFS_gcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalación de Hadoop\n",
        "\n",
        "Hadoop es un marco de programación basado en Java que permite procesar y almacenar conjuntos de datos extremadamente grandes en un clúster de máquinas de bajo coste. Fue el primer gran proyecto de código abierto en el ámbito del Big Data y está patrocinado por la Apache Software Foundation."
      ],
      "metadata": {
        "id": "TTvVoompp8-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 1: Instalación de Haddoop"
      ],
      "metadata": {
        "id": "U7Cr4W9PqY1x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U26D-ATvp8Jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc341a3-7def-4630-8051-e29e0db82991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-17 15:35:30--  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.4.tar.gz.2’\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M   808KB/s    in 17m 46s \n",
            "\n",
            "2026-02-17 15:53:17 (637 KB/s) - ‘hadoop-3.3.4.tar.gz.2’ saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizaremos el comando tar con los parámetros:\n",
        "\n",
        "\n",
        "-x para extraer,\n",
        "\n",
        "-z para descomprimir,\n",
        "\n",
        "-f para especificar que estamos extrayendo de un archivo\n",
        "\n",
        "(podremos añadir -v para la salida en detalle)"
      ],
      "metadata": {
        "id": "T74g5GHjs3Yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.4.tar.gz"
      ],
      "metadata": {
        "id": "wSkE8_HwtDW8",
        "outputId": "31f8fcb5-6ff7-461c-b6ba-892876f7b4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copiamos el directorio de hadoop en /usr/local"
      ],
      "metadata": {
        "id": "aVW9Vy-2tQ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r hadoop-3.3.4 /usr/local/"
      ],
      "metadata": {
        "id": "hLiYlo_CtbxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 2: Configurar el Hadoop JAVA HOME\n",
        "Hadoop requiere que se establezca la ruta de acceso a Java, ya sea como una variable de entorno o en el archivo de configuración de Hadoop."
      ],
      "metadata": {
        "id": "etL3_KXZtol8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Buscamos cual es la dirección de Java en la máquina Google Colab\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7-XQYmoFtvHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "id": "N1i-objMt7_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f772fb2d-0e88-4ed0-dcd4-adfa45cfa878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-17-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Establecemos mediante código Python el valor de esta variable\n",
        "\n"
      ],
      "metadata": {
        "id": "eGbpJfYNuHVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\""
      ],
      "metadata": {
        "id": "5XTXjmfcuTR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paso 3: Ejecutando Hadoop"
      ],
      "metadata": {
        "id": "EFT_a4toufMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos la versión de Hadoop:"
      ],
      "metadata": {
        "id": "-N3opWUDuxa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "id": "Rov1p_17cjfp",
        "outputId": "4adb6d23-30e7-476c-d461-98d768a34172",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rwxr-xr-x 1 root root 9034 Feb 17 16:30 /usr/local/hadoop-3.3.4/bin/hadoop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Borramos intentos fallidos previos\n",
        "!rm -rf hadoop-3.3.4.tar.gz* !rm -rf /usr/local/hadoop-3.3.4\n",
        "\n",
        "# 2. Descargamos de nuevo (usando un mirror fiable)\n",
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
        "\n",
        "# 3. Extraemos (esto tardará un poco, asegúrate de que NO salga \"Unexpected EOF\")\n",
        "!tar -xzf hadoop-3.3.4.tar.gz\n",
        "\n",
        "# 4. Movemos a la ruta definitiva\n",
        "!cp -r hadoop-3.3.4 /usr/local/"
      ],
      "metadata": {
        "id": "6Y1Mge-2ZjqM",
        "outputId": "1f96fc60-3aca-42ac-a3a7-e14f472df6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-17 16:19:40--  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 695457782 (663M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.4.tar.gz’\n",
            "\n",
            "hadoop-3.3.4.tar.gz 100%[===================>] 663.24M  1.46MB/s    in 10m 12s \n",
            "\n",
            "2026-02-17 16:29:53 (1.08 MB/s) - ‘hadoop-3.3.4.tar.gz’ saved [695457782/695457782]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar grep ~/input ~/grep_example 'allowed[.]*'"
      ],
      "metadata": {
        "id": "cHOX1ZHOuzCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57725f2e-f335-4c06-8406-759ab6b03650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-02-17 16:31:26,064 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2026-02-17 16:31:26,304 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2026-02-17 16:31:26,304 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2026-02-17 16:31:26,555 INFO input.FileInputFormat: Total input files to process : 0\n",
            "2026-02-17 16:31:26,568 INFO mapreduce.JobSubmitter: number of splits:0\n",
            "2026-02-17 16:31:27,013 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1454203266_0001\n",
            "2026-02-17 16:31:27,013 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-17 16:31:27,269 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-17 16:31:27,270 INFO mapreduce.Job: Running job: job_local1454203266_0001\n",
            "2026-02-17 16:31:27,272 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-17 16:31:27,283 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:31:27,283 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:31:27,285 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2026-02-17 16:31:27,330 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-17 16:31:27,330 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-17 16:31:27,340 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-17 16:31:27,340 INFO mapred.LocalJobRunner: Starting task: attempt_local1454203266_0001_r_000000_0\n",
            "2026-02-17 16:31:27,374 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:31:27,374 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:31:27,398 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:31:27,404 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4e867056\n",
            "2026-02-17 16:31:27,406 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-17 16:31:27,438 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-17 16:31:27,443 INFO reduce.EventFetcher: attempt_local1454203266_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-17 16:31:27,459 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-17 16:31:27,463 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:31:27,464 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-17 16:31:27,464 INFO reduce.MergeManagerImpl: Merging 0 files, 0 bytes from disk\n",
            "2026-02-17 16:31:27,465 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-17 16:31:27,468 INFO mapred.Merger: Merging 0 sorted segments\n",
            "2026-02-17 16:31:27,469 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
            "2026-02-17 16:31:27,469 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:31:27,494 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-17 16:31:27,512 INFO mapred.Task: Task:attempt_local1454203266_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 16:31:27,519 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:31:27,519 INFO mapred.Task: Task attempt_local1454203266_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-17 16:31:27,523 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1454203266_0001_r_000000_0' to file:/content/grep-temp-450807679\n",
            "2026-02-17 16:31:27,526 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-17 16:31:27,526 INFO mapred.Task: Task 'attempt_local1454203266_0001_r_000000_0' done.\n",
            "2026-02-17 16:31:27,538 INFO mapred.Task: Final Counters for attempt_local1454203266_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281012\n",
            "\t\tFILE: Number of bytes written=919422\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=0\n",
            "\t\tReduce shuffle bytes=0\n",
            "\t\tReduce input records=0\n",
            "\t\tReduce output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tShuffled Maps =0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=218103808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=98\n",
            "2026-02-17 16:31:27,538 INFO mapred.LocalJobRunner: Finishing task: attempt_local1454203266_0001_r_000000_0\n",
            "2026-02-17 16:31:27,539 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-17 16:31:28,280 INFO mapreduce.Job: Job job_local1454203266_0001 running in uber mode : false\n",
            "2026-02-17 16:31:28,282 INFO mapreduce.Job:  map 0% reduce 100%\n",
            "2026-02-17 16:31:28,283 INFO mapreduce.Job: Job job_local1454203266_0001 completed successfully\n",
            "2026-02-17 16:31:28,290 INFO mapreduce.Job: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=281012\n",
            "\t\tFILE: Number of bytes written=919422\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=0\n",
            "\t\tReduce shuffle bytes=0\n",
            "\t\tReduce input records=0\n",
            "\t\tReduce output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tShuffled Maps =0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=218103808\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=98\n",
            "2026-02-17 16:31:28,329 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-17 16:31:28,353 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2026-02-17 16:31:28,361 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-17 16:31:28,388 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local165032844_0002\n",
            "2026-02-17 16:31:28,388 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-17 16:31:28,543 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-17 16:31:28,543 INFO mapreduce.Job: Running job: job_local165032844_0002\n",
            "2026-02-17 16:31:28,544 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-17 16:31:28,544 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:31:28,544 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:31:28,545 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2026-02-17 16:31:28,552 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-17 16:31:28,552 INFO mapred.LocalJobRunner: Starting task: attempt_local165032844_0002_m_000000_0\n",
            "2026-02-17 16:31:28,557 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:31:28,558 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:31:28,558 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:31:28,562 INFO mapred.MapTask: Processing split: file:/content/grep-temp-450807679/part-r-00000:0+86\n",
            "2026-02-17 16:31:28,676 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:31:28,677 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:31:28,677 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:31:28,677 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:31:28,677 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:31:28,680 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:31:28,689 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:31:28,690 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:31:28,709 INFO mapred.Task: Task:attempt_local165032844_0002_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 16:31:28,714 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:31:28,714 INFO mapred.Task: Task 'attempt_local165032844_0002_m_000000_0' done.\n",
            "2026-02-17 16:31:28,717 INFO mapred.Task: Final Counters for attempt_local165032844_0002_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=562265\n",
            "\t\tFILE: Number of bytes written=1834537\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=0\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=111\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=98\n",
            "2026-02-17 16:31:28,717 INFO mapred.LocalJobRunner: Finishing task: attempt_local165032844_0002_m_000000_0\n",
            "2026-02-17 16:31:28,718 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-17 16:31:28,721 INFO mapred.LocalJobRunner: Starting task: attempt_local165032844_0002_r_000000_0\n",
            "2026-02-17 16:31:28,722 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-17 16:31:28,723 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:31:28,723 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:31:28,724 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:31:28,724 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72aedf84\n",
            "2026-02-17 16:31:28,724 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-17 16:31:28,726 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-17 16:31:28,729 INFO reduce.EventFetcher: attempt_local165032844_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-17 16:31:28,775 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local165032844_0002_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:31:28,780 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local165032844_0002_m_000000_0\n",
            "2026-02-17 16:31:28,780 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
            "2026-02-17 16:31:28,782 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-17 16:31:28,783 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 16:31:28,785 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-17 16:31:28,788 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-17 16:31:28,789 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
            "2026-02-17 16:31:28,790 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-17 16:31:28,791 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
            "2026-02-17 16:31:28,791 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-17 16:31:28,791 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-17 16:31:28,792 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
            "2026-02-17 16:31:28,793 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 16:31:28,798 INFO mapred.Task: Task:attempt_local165032844_0002_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 16:31:28,799 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 16:31:28,799 INFO mapred.Task: Task attempt_local165032844_0002_r_000000_0 is allowed to commit now\n",
            "2026-02-17 16:31:28,801 INFO output.FileOutputCommitter: Saved output of task 'attempt_local165032844_0002_r_000000_0' to file:/root/grep_example\n",
            "2026-02-17 16:31:28,802 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-17 16:31:28,802 INFO mapred.Task: Task 'attempt_local165032844_0002_r_000000_0' done.\n",
            "2026-02-17 16:31:28,803 INFO mapred.Task: Final Counters for attempt_local165032844_0002_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=562309\n",
            "\t\tFILE: Number of bytes written=1834551\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=0\n",
            "\t\tReduce shuffle bytes=6\n",
            "\t\tReduce input records=0\n",
            "\t\tReduce output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=8\n",
            "2026-02-17 16:31:28,803 INFO mapred.LocalJobRunner: Finishing task: attempt_local165032844_0002_r_000000_0\n",
            "2026-02-17 16:31:28,803 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-17 16:31:29,544 INFO mapreduce.Job: Job job_local165032844_0002 running in uber mode : false\n",
            "2026-02-17 16:31:29,545 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-17 16:31:29,545 INFO mapreduce.Job: Job job_local165032844_0002 completed successfully\n",
            "2026-02-17 16:31:29,549 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1124574\n",
            "\t\tFILE: Number of bytes written=3669088\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=0\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=111\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=0\n",
            "\t\tReduce shuffle bytes=6\n",
            "\t\tReduce input records=0\n",
            "\t\tReduce output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=524288000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=98\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ~/grep_example"
      ],
      "metadata": {
        "id": "vxPEVJmNZGXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecutamos el comando sin parámetros:"
      ],
      "metadata": {
        "id": "MkePyDe9u52N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "id": "q7qd8O3Du7iK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5561c7c-a073-4fcb-a805-9f6598b30e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una de las formas tradicionales de asegurarnos que un ambiente de Hadoop recién instalado funciona correctamente, es ejecutar el jar de ejemplos map-reduce incluido con todas las distribucioines de hadoop (hadoop-mapreduce-examples.jar)."
      ],
      "metadata": {
        "id": "phr6wo-EvGUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Creamos un directorio llamado input en nuestro directorio de inicio"
      ],
      "metadata": {
        "id": "RpXKshVyvT_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/input"
      ],
      "metadata": {
        "id": "2mPXXbQwvM4c",
        "outputId": "34e7343e-3b7c-4d48-f7fb-87d0725aa20e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/input’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Copiamos los archivos de configuración (los xml) de Hadoop para usar esos archivos como nuestros datos de entrada."
      ],
      "metadata": {
        "id": "z3gRhT8wvgDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /usr/local/hadoop-3.3.4/etc/hadoop/*.xml ~/input\n",
        "!ls ~/input"
      ],
      "metadata": {
        "id": "-_jS_Aq8vr8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516c6f0d-ad16-4703-b823-9e4498b0f764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\thdfs-rbf-site.xml  kms-acls.xml     yarn-site.xml\n",
            "core-site.xml\t\thdfs-site.xml\t   kms-site.xml\n",
            "hadoop-policy.xml\thttpfs-site.xml    mapred-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Ejecutamos hadoop jar con el fin de ejecutar uno de los ejemplos por defecto, en este caso el grep que busca expresiones regulares dentro de los ficheros que le especifiquemos."
      ],
      "metadata": {
        "id": "tBdfkHoTvx4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   /usr/local/hadoop/bin/hadoop Es el directorio donde esta el ejecutable de hadoop en el sistema.\n",
        "*   jar Le indica a hadoop que deseamos ejecutar una aplicacion empaquetada de Java. (Jar)\n",
        "*   /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar Es la ruta donde esta el Jar que deseamos ejecutar. La versión del jar depende de la versión de hadoop instalada.\n",
        "*   grep Es un parámetro de los muchos que se le pueden pasar al Jar de ejemplos que trae Hadoop. grep sirve para encontrar y contar ocurrencias de strings haciendo uso de expresiones regulares.\n",
        "*   ~/input El directorio de entrada. Es donde el programa va a buscar los archivos de entrada a la tarea de map-reduce. Aquí copiamos unos archivos de prueba en un comando anterior.\n",
        "*   ~/grep_example El directorio de salida. Es donde el programa va a escribir el resultado de la corrida de la aplicación. En este caso, la cantidad de veces que la palabra del parámetro siguiente, aparece en los archivos de entrada.\n",
        "*   ‘allowed[.]*’ Es la expresión regular que deseamos buscar."
      ],
      "metadata": {
        "id": "eo35loL7v-Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.   Los resultados se almacenan en el directorio de salida (~/grep_example/) y se pueden verificar ejecutando cat el directorio de salida:\n",
        "\n"
      ],
      "metadata": {
        "id": "uU55dsBMwpWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo chown -R $USER:$USER /usr/local/hadoop-3.3.4\n",
        "!chmod +x /usr/local/hadoop-3.3.4/bin/hadoop"
      ],
      "metadata": {
        "id": "mPoVEH_sXH0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hadoop jar \\\n",
        "  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\n",
        "  grep ~/input ~/grep_example 'allowed[.]*'"
      ],
      "metadata": {
        "id": "HHcNVQyPv63K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65d82a9b-a8d5-4ad9-e22f-6c0883c6c01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026-02-17 16:35:07,859 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2026-02-17 16:35:08,056 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2026-02-17 16:35:08,056 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2026-02-17 16:35:08,303 INFO input.FileInputFormat: Total input files to process : 10\n",
            "2026-02-17 16:35:08,346 INFO mapreduce.JobSubmitter: number of splits:10\n",
            "2026-02-17 16:35:08,738 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1912950419_0001\n",
            "2026-02-17 16:35:08,738 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-17 16:35:08,989 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-17 16:35:08,990 INFO mapreduce.Job: Running job: job_local1912950419_0001\n",
            "2026-02-17 16:35:08,992 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-17 16:35:09,003 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,003 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,004 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2026-02-17 16:35:09,076 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000000_0\n",
            "2026-02-17 16:35:09,078 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-17 16:35:09,108 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,108 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,134 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,142 INFO mapred.MapTask: Processing split: file:/root/input/hadoop-policy.xml:0+11765\n",
            "2026-02-17 16:35:09,318 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,319 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,319 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,319 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,319 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,325 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,361 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,362 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,362 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-17 16:35:09,362 INFO mapred.MapTask: bufstart = 0; bufend = 374; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,362 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214312(104857248); length = 85/6553600\n",
            "2026-02-17 16:35:09,377 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-17 16:35:09,390 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,393 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,394 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000000_0' done.\n",
            "2026-02-17 16:35:09,404 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=293934\n",
            "\t\tFILE: Number of bytes written=920525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=275\n",
            "\t\tMap output records=22\n",
            "\t\tMap output bytes=374\n",
            "\t\tMap output materialized bytes=25\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=22\n",
            "\t\tCombine output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=18\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=11765\n",
            "2026-02-17 16:35:09,404 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000000_0\n",
            "2026-02-17 16:35:09,406 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000001_0\n",
            "2026-02-17 16:35:09,411 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,411 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,412 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,417 INFO mapred.MapTask: Processing split: file:/root/input/capacity-scheduler.xml:0+9213\n",
            "2026-02-17 16:35:09,453 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,453 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,453 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,453 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,453 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,456 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,464 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,465 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,465 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-17 16:35:09,465 INFO mapred.MapTask: bufstart = 0; bufend = 16; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,465 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2026-02-17 16:35:09,467 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-17 16:35:09,470 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000001_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,472 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,474 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000001_0' done.\n",
            "2026-02-17 16:35:09,475 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000001_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=304139\n",
            "\t\tFILE: Number of bytes written=920581\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=244\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=16\n",
            "\t\tMap output materialized bytes=24\n",
            "\t\tInput split bytes=104\n",
            "\t\tCombine input records=1\n",
            "\t\tCombine output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=16\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=9213\n",
            "2026-02-17 16:35:09,475 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000001_0\n",
            "2026-02-17 16:35:09,476 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000002_0\n",
            "2026-02-17 16:35:09,478 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,478 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,478 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,480 INFO mapred.MapTask: Processing split: file:/root/input/kms-acls.xml:0+3518\n",
            "2026-02-17 16:35:09,512 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,513 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,513 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,513 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,513 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,514 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,521 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,521 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,525 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000002_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,527 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,527 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000002_0' done.\n",
            "2026-02-17 16:35:09,528 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000002_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=308649\n",
            "\t\tFILE: Number of bytes written=920619\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=135\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=14\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=3518\n",
            "2026-02-17 16:35:09,528 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000002_0\n",
            "2026-02-17 16:35:09,528 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000003_0\n",
            "2026-02-17 16:35:09,530 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,530 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,530 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,532 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-site.xml:0+775\n",
            "2026-02-17 16:35:09,569 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,569 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,570 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,570 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,570 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,576 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,584 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,584 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,600 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000003_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,607 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,609 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000003_0' done.\n",
            "2026-02-17 16:35:09,610 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000003_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=310416\n",
            "\t\tFILE: Number of bytes written=920657\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=12\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=775\n",
            "2026-02-17 16:35:09,613 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000003_0\n",
            "2026-02-17 16:35:09,613 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000004_0\n",
            "2026-02-17 16:35:09,617 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,617 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,618 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,626 INFO mapred.MapTask: Processing split: file:/root/input/core-site.xml:0+774\n",
            "2026-02-17 16:35:09,663 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,663 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,663 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,663 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,664 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,665 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,669 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,670 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,673 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000004_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,675 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,676 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000004_0' done.\n",
            "2026-02-17 16:35:09,676 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000004_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=312182\n",
            "\t\tFILE: Number of bytes written=920695\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=774\n",
            "2026-02-17 16:35:09,676 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000004_0\n",
            "2026-02-17 16:35:09,677 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000005_0\n",
            "2026-02-17 16:35:09,678 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,678 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,679 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,680 INFO mapred.MapTask: Processing split: file:/root/input/mapred-site.xml:0+758\n",
            "2026-02-17 16:35:09,791 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,792 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,792 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,792 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,792 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,793 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,800 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,800 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,805 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000005_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,808 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,808 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000005_0' done.\n",
            "2026-02-17 16:35:09,810 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000005_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=313932\n",
            "\t\tFILE: Number of bytes written=920733\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=21\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=97\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=758\n",
            "2026-02-17 16:35:09,810 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000005_0\n",
            "2026-02-17 16:35:09,811 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000006_0\n",
            "2026-02-17 16:35:09,812 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,812 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,813 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,818 INFO mapred.MapTask: Processing split: file:/root/input/yarn-site.xml:0+690\n",
            "2026-02-17 16:35:09,858 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,858 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,858 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,859 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,859 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,864 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,871 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,872 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,877 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000006_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,881 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,881 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000006_0' done.\n",
            "2026-02-17 16:35:09,883 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000006_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=315102\n",
            "\t\tFILE: Number of bytes written=920771\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=95\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=19\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=690\n",
            "2026-02-17 16:35:09,883 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000006_0\n",
            "2026-02-17 16:35:09,883 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000007_0\n",
            "2026-02-17 16:35:09,885 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,885 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,887 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,889 INFO mapred.MapTask: Processing split: file:/root/input/hdfs-rbf-site.xml:0+683\n",
            "2026-02-17 16:35:09,907 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,908 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,908 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,908 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,908 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,910 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,914 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,914 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,918 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000007_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,920 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,920 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000007_0' done.\n",
            "2026-02-17 16:35:09,921 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000007_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=316265\n",
            "\t\tFILE: Number of bytes written=920809\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=99\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=683\n",
            "2026-02-17 16:35:09,921 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000007_0\n",
            "2026-02-17 16:35:09,921 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000008_0\n",
            "2026-02-17 16:35:09,923 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,923 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,923 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,925 INFO mapred.MapTask: Processing split: file:/root/input/kms-site.xml:0+682\n",
            "2026-02-17 16:35:09,964 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,964 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,964 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,964 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,964 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,965 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:09,970 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:09,970 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:09,974 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000008_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:09,975 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:09,976 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000008_0' done.\n",
            "2026-02-17 16:35:09,976 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000008_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=317427\n",
            "\t\tFILE: Number of bytes written=920847\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=20\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=94\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=682\n",
            "2026-02-17 16:35:09,976 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000008_0\n",
            "2026-02-17 16:35:09,976 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_m_000009_0\n",
            "2026-02-17 16:35:09,978 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:09,978 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:09,979 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:09,980 INFO mapred.MapTask: Processing split: file:/root/input/httpfs-site.xml:0+620\n",
            "2026-02-17 16:35:09,997 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 16:35:09,997 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 16:35:09,997 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 16:35:09,998 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 16:35:09,998 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 16:35:09,999 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 16:35:10,001 INFO mapreduce.Job: Job job_local1912950419_0001 running in uber mode : false\n",
            "2026-02-17 16:35:10,003 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-17 16:35:10,008 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 16:35:10,009 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 16:35:10,012 INFO mapred.Task: Task:attempt_local1912950419_0001_m_000009_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:10,014 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 16:35:10,014 INFO mapred.Task: Task 'attempt_local1912950419_0001_m_000009_0' done.\n",
            "2026-02-17 16:35:10,015 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_m_000009_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318527\n",
            "\t\tFILE: Number of bytes written=920885\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=17\n",
            "\t\tMap output records=0\n",
            "\t\tMap output bytes=0\n",
            "\t\tMap output materialized bytes=6\n",
            "\t\tInput split bytes=97\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=620\n",
            "2026-02-17 16:35:10,015 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_m_000009_0\n",
            "2026-02-17 16:35:10,015 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-17 16:35:10,021 INFO mapred.LocalJobRunner: Starting task: attempt_local1912950419_0001_r_000000_0\n",
            "2026-02-17 16:35:10,023 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-17 16:35:10,030 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 16:35:10,031 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 16:35:10,031 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 16:35:10,034 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2ee07e8a\n",
            "2026-02-17 16:35:10,036 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-17 16:35:10,064 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-17 16:35:10,069 INFO reduce.EventFetcher: attempt_local1912950419_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-17 16:35:10,117 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000006_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,123 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000006_0\n",
            "2026-02-17 16:35:10,125 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
            "2026-02-17 16:35:10,130 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000003_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,133 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000003_0\n",
            "2026-02-17 16:35:10,133 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->4\n",
            "2026-02-17 16:35:10,136 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000009_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,137 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000009_0\n",
            "2026-02-17 16:35:10,138 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 3, commitMemory -> 4, usedMemory ->6\n",
            "2026-02-17 16:35:10,140 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000007_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,142 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000007_0\n",
            "2026-02-17 16:35:10,142 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 4, commitMemory -> 6, usedMemory ->8\n",
            "2026-02-17 16:35:10,145 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000000_0 decomp: 21 len: 25 to MEMORY\n",
            "2026-02-17 16:35:10,145 INFO reduce.InMemoryMapOutput: Read 21 bytes from map-output for attempt_local1912950419_0001_m_000000_0\n",
            "2026-02-17 16:35:10,145 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 21, inMemoryMapOutputs.size() -> 5, commitMemory -> 8, usedMemory ->29\n",
            "2026-02-17 16:35:10,147 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000004_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,148 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000004_0\n",
            "2026-02-17 16:35:10,148 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 6, commitMemory -> 29, usedMemory ->31\n",
            "2026-02-17 16:35:10,150 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000001_0 decomp: 20 len: 24 to MEMORY\n",
            "2026-02-17 16:35:10,151 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local1912950419_0001_m_000001_0\n",
            "2026-02-17 16:35:10,151 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 7, commitMemory -> 31, usedMemory ->51\n",
            "2026-02-17 16:35:10,153 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000002_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,154 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000002_0\n",
            "2026-02-17 16:35:10,154 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 8, commitMemory -> 51, usedMemory ->53\n",
            "2026-02-17 16:35:10,156 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000008_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,156 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000008_0\n",
            "2026-02-17 16:35:10,157 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 9, commitMemory -> 53, usedMemory ->55\n",
            "2026-02-17 16:35:10,159 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1912950419_0001_m_000005_0 decomp: 2 len: 6 to MEMORY\n",
            "2026-02-17 16:35:10,159 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local1912950419_0001_m_000005_0\n",
            "2026-02-17 16:35:10,159 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 10, commitMemory -> 55, usedMemory ->57\n",
            "2026-02-17 16:35:10,162 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-17 16:35:10,163 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2026-02-17 16:35:10,163 INFO reduce.MergeManagerImpl: finalMerge called with 10 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-17 16:35:10,175 INFO mapred.Merger: Merging 10 sorted segments\n",
            "2026-02-17 16:35:10,176 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 20 bytes\n",
            "2026-02-17 16:35:10,178 INFO reduce.MergeManagerImpl: Merged 10 segments, 57 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-17 16:35:10,179 INFO reduce.MergeManagerImpl: Merging 1 files, 43 bytes from disk\n",
            "2026-02-17 16:35:10,180 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-17 16:35:10,181 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-17 16:35:10,182 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 29 bytes\n",
            "2026-02-17 16:35:10,182 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2026-02-17 16:35:10,201 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-17 16:35:10,205 INFO mapred.Task: Task:attempt_local1912950419_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 16:35:10,207 INFO mapred.LocalJobRunner: 10 / 10 copied.\n",
            "2026-02-17 16:35:10,207 INFO mapred.Task: Task attempt_local1912950419_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-17 16:35:10,209 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1912950419_0001_r_000000_0' to file:/content/grep-temp-1426617672\n",
            "2026-02-17 16:35:10,210 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-17 16:35:10,211 INFO mapred.Task: Task 'attempt_local1912950419_0001_r_000000_0' done.\n",
            "2026-02-17 16:35:10,212 INFO mapred.Task: Final Counters for attempt_local1912950419_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=318987\n",
            "\t\tFILE: Number of bytes written=921075\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=97\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=10\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=545259520\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=147\n",
            "2026-02-17 16:35:10,213 INFO mapred.LocalJobRunner: Finishing task: attempt_local1912950419_0001_r_000000_0\n",
            "2026-02-17 16:35:10,213 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-17 16:35:11,008 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-17 16:35:11,009 INFO mapreduce.Job: Job job_local1912950419_0001 completed successfully\n",
            "2026-02-17 16:35:11,024 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3429560\n",
            "\t\tFILE: Number of bytes written=10128197\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=792\n",
            "\t\tMap output records=23\n",
            "\t\tMap output bytes=390\n",
            "\t\tMap output materialized bytes=97\n",
            "\t\tInput split bytes=969\n",
            "\t\tCombine input records=23\n",
            "\t\tCombine output records=2\n",
            "\t\tReduce input groups=2\n",
            "\t\tReduce shuffle bytes=97\n",
            "\t\tReduce input records=2\n",
            "\t\tReduce output records=2\n",
            "\t\tSpilled Records=4\n",
            "\t\tShuffled Maps =10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=10\n",
            "\t\tGC time elapsed (ms)=109\n",
            "\t\tTotal committed heap usage (bytes)=4865392640\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=29478\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=147\n",
            "2026-02-17 16:35:11,054 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/root/grep_example already exists\n",
            "\tat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:164)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:277)\n",
            "\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1571)\n",
            "\tat org.apache.hadoop.mapreduce.Job$11.run(Job.java:1568)\n",
            "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:712)\n",
            "\tat java.base/javax.security.auth.Subject.doAs(Subject.java:439)\n",
            "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)\n",
            "\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1568)\n",
            "\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1589)\n",
            "\tat org.apache.hadoop.examples.Grep.run(Grep.java:94)\n",
            "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81)\n",
            "\tat org.apache.hadoop.examples.Grep.main(Grep.java:103)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n",
            "\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n",
            "\tat org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
            "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\n",
            "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "Command 'b\"/usr/local/hadoop-3.3.4/bin/hadoop jar \\\\\\n  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\\\\n  grep ~/input ~/grep_example 'allowed[.]*'\\n\"' returned non-zero exit status 255.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-87514201.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/usr/local/hadoop-3.3.4/bin/hadoop jar \\\\\\n  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\\\\n  grep ~/input ~/grep_example 'allowed[.]*'\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"/usr/local/hadoop-3.3.4/bin/hadoop jar \\\\\\n  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\\\\n  grep ~/input ~/grep_example 'allowed[.]*'\\n\"' returned non-zero exit status 255."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/grep_example/*"
      ],
      "metadata": {
        "id": "R-VTuKtnwo5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es interesante probar otros ejemplos contenidos en el jar de ejemplos de hadoop. [Hadoop Map Reduce Examples](http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/)\n",
        "\n",
        "Puedes ejecutar el siguiente comando para obtener una referencia de los ejemplos disponibles en esta distribución"
      ],
      "metadata": {
        "id": "RvjHvQlww0JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B29TgZAsOD8",
        "outputId": "7a124090-1301-42a8-8c19-3384b260c16e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An example program must be given as the first argument.\n",
            "Valid program names are:\n",
            "  aggregatewordcount: An Aggregate based map/reduce program that counts the words in the input files.\n",
            "  aggregatewordhist: An Aggregate based map/reduce program that computes the histogram of the words in the input files.\n",
            "  bbp: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi.\n",
            "  dbcount: An example job that count the pageview counts from a database.\n",
            "  distbbp: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi.\n",
            "  grep: A map/reduce program that counts the matches of a regex in the input.\n",
            "  join: A job that effects a join over sorted, equally partitioned datasets\n",
            "  multifilewc: A job that counts words from several files.\n",
            "  pentomino: A map/reduce tile laying program to find solutions to pentomino problems.\n",
            "  pi: A map/reduce program that estimates Pi using a quasi-Monte Carlo method.\n",
            "  randomtextwriter: A map/reduce program that writes 10GB of random textual data per node.\n",
            "  randomwriter: A map/reduce program that writes 10GB of random data per node.\n",
            "  secondarysort: An example defining a secondary sort to the reduce.\n",
            "  sort: A map/reduce program that sorts the data written by the random writer.\n",
            "  sudoku: A sudoku solver.\n",
            "  teragen: Generate data for the terasort\n",
            "  terasort: Run the terasort\n",
            "  teravalidate: Checking results of terasort\n",
            "  wordcount: A map/reduce program that counts the words in the input files.\n",
            "  wordmean: A map/reduce program that counts the average length of the words in the input files.\n",
            "  wordmedian: A map/reduce program that counts the median length of the words in the input files.\n",
            "  wordstandarddeviation: A map/reduce program that counts the standard deviation of the length of the words in the input files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS\n",
        "\n",
        "- Es un sistema de archivos distribuido.\n",
        "- HDFS es altamente tolerante a fallos y está diseñado para ser desplegado en hardware de bajo coste.\n",
        "- HDFS es adecuado para aplicaciones que manejan grandes conjuntos de datos.\n",
        "- HDFS proporciona interfaces para acercar las aplicaciones al lugar donde se encuentran los datos. El cálculo es mucho más eficiente cuando el tamaño del conjunto de datos es enorme.\n",
        "- HDFS consiste en un único NameNode con un número de DataNodes que gestionan el almacenamiento.\n",
        "- HDFS dispone de un espacio de nombres del sistema de archivos y permite que los datos del usuario se almacenen en archivos.\n",
        "  1. El NameNode divide un archivo en bloques almacenados en DataNodes.\n",
        "  2. El NameNode ejecuta operaciones como abrir, cerrar y renombrar archivos y directorios.\n",
        "  3. El NameNode secundario almacena la información del NameNode.\n",
        "  4.  Los DataNodes gestionan la creación, eliminación y replicación de bloques siguiendo instrucciones del NameNode.\n",
        "  5. La ubicación de las réplicas está optimizada para la fiabilidad de los datos, la disponibilidad y la utilización del ancho de banda de la red.\n",
        "  6. Los datos del usuario nunca pasan por el NameNode.\n",
        "\n",
        "- Los archivos en HDFS son de una sola escritura y tienen estrictamente un único proceso escritor en cualquier momento.\n",
        "\n",
        "- El DataNode no tiene conocimiento de los archivos de HDFS."
      ],
      "metadata": {
        "id": "UO1DRA0jvnzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q1YK37bBmAlc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTA: Las siguientes sentencias únicamente sirven para probar comandos básicos de HDFS no para gestionar una Infraestructura que en Google Colab no existe, en este caso el sistema de archivos HDFS es el mismo que el local**"
      ],
      "metadata": {
        "id": "dJbfP6ZRvyKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accesibilidad\n",
        "\n",
        "Todos los  [comandos HDFS](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/FileSystemShell.html)  se invocan mediante el script *bin/hdfs* :\n",
        "```shell\n",
        "hdfs [SHELL_OPTIONS] COMMAND [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
        "```\n",
        "## Gestión de ficheros y directorios\n",
        "```shell\n",
        "hdfs dfs -ls -h -R # lista de subdirectorios recuriva\n",
        "hdfs dfs -cp  # Copia de ficheros\n",
        "hdfs dfs -mv  # Movimiento de ficheros\n",
        "hdfs dfs -mkdir /foodir # Crear directorio llamado /foodir\n",
        "hdfs dfs -rm -r /foodir   # Borrar un directorio /foodir\n",
        "hdfs dfs -cat /foodir/myfile.txt # Ver los contenidos de un fichero /foodir/myfile.txt\n",
        "```"
      ],
      "metadata": {
        "id": "HfPCzSN6v6P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Listar el contenido de un directorio\n"
      ],
      "metadata": {
        "id": "ChwKiwl_wmLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvLjTqGYkDgp",
        "outputId": "966704e4-f9c8-4cd1-ba57-444c82eb5824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 items\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:24 /content/.config\n",
            "drwxr-xr-x   - 1024 1024       4096 2022-07-29 13:44 /content/hadoop-3.3.4\n",
            "-rw-r--r--   1 root root  695457782 2022-07-29 18:11 /content/hadoop-3.3.4.tar.gz\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:24 /content/sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Crear el directorio prueba"
      ],
      "metadata": {
        "id": "7pdcP5rdw3e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir prueba\n",
        "/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q67sE80mmibi",
        "outputId": "c015ee9f-e737-4363-cb14-07bde1794d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 items\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:24 .config\n",
            "drwxr-xr-x   - 1024 1024       4096 2022-07-29 13:44 hadoop-3.3.4\n",
            "-rw-r--r--   1 root root  695457782 2022-07-29 18:11 hadoop-3.3.4.tar.gz\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 16:37 prueba\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:24 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Copar en HDFS:\n",
        "\n",
        "```bash\n",
        "hdfs dfs -put user.txt\n",
        "```\n",
        "\n",
        "*   Comprobar:\n",
        "```bash\n",
        "hdfs dfs -ls -R\n",
        "hdfs dfs -cat user.txt\n",
        "hdfs dfs -tail user.txt\n",
        "```\n",
        "- Crear un fichero local :"
      ],
      "metadata": {
        "id": "uUEc8iWXxkSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "echo \"Ejemplo de HDFS\" > user.txt\n",
        "echo `date` >> user.txt\n",
        "cat user.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xowJIi-yztW",
        "outputId": "2564417e-3b48-4a56-cb10-f1d083b3b980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de HDFS\n",
            "Tue Feb 17 04:37:23 PM UTC 2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Colocarlo (copiarlo)  en el directorio *prueba*"
      ],
      "metadata": {
        "id": "CssXQcnLy34L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put user.txt prueba/"
      ],
      "metadata": {
        "id": "c-Z6SfyHy4jQ",
        "outputId": "a08f747e-2511-4ba6-ac85-5f5656ce30c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `prueba/user.txt': File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mostrar su contenido"
      ],
      "metadata": {
        "id": "an7ImoIqzBBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cat prueba/user.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rN43iUIzBwb",
        "outputId": "8a9c6018-98ea-4f86-de78-b8c6baf07522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ejemplo de HDFS\n",
            "Tue Feb 17 04:37:23 PM UTC 2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1\n",
        "1. Cree un directorio `files` en HDFS.\n",
        "2. Listar el contenido de un directorio /.\n",
        "3. Cargar el archivo today.txt en HDFS.\n",
        "```bash\n",
        "date > hoy.txt\n",
        "whoami >> hoy.txt\n",
        "```\n",
        "4. Mostrar el contenido del archivo `hoy.txt`.\n",
        "5. Copiar el archivo `hoy.txt` del origen al directorio `files`.\n",
        "6. Copiar el archivo `jps.txt` desde/hacia el sistema de archivos local a HDFS\n",
        "```bash\n",
        "jps > jps.txt\n",
        "```\n",
        "7. Mover el archivo `jps.txt` de la fuente a `files`.\n",
        "8. Eliminar el archivo `today.txt` del directorio principal en HDFS.\n",
        "9. Mostrar las últimas líneas de `jps.txt`.\n"
      ],
      "metadata": {
        "id": "kdpKi-H_zPbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear directorio files en HDFS\n",
        "!date > hoy.txt\n",
        "!whoami >> hoy.txt\n",
        "!jps > jps.txt"
      ],
      "metadata": {
        "id": "w_ezwdHeeEva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificamos que existen en local\n",
        "!ls hoy.txt jps.txt"
      ],
      "metadata": {
        "id": "eABoCpAGfB8o",
        "outputId": "788beb0e-8a38-4e19-95cc-faffa5622440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hoy.txt  jps.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir -p /user/root/files"
      ],
      "metadata": {
        "id": "93JLAp0ffNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Listar el contenido para ver si se ha creado\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /user/root/"
      ],
      "metadata": {
        "id": "gMy9j3tHfRNc",
        "outputId": "b0a65306-1577-4521-ab81-d20df1e98cf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 16:41 /user/root/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listar el contenido\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /"
      ],
      "metadata": {
        "id": "zCZ6tFt2gQtG",
        "outputId": "89f4c5e5-437b-434a-cf9c-4162f3172558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29 items\n",
            "-rwxr-xr-x   1 root root          0 2026-02-17 15:12 /.dockerenv\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:11 /bin\n",
            "drwxr-xr-x   - root root       4096 2022-04-18 10:28 /boot\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 16:43 /content\n",
            "drwxr-xr-x   - root root       4096 2026-02-06 14:15 /datalab\n",
            "drwxr-xr-x   - root root        360 2026-02-17 15:13 /dev\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 15:12 /etc\n",
            "drwxr-xr-x   - root root       4096 2022-04-18 10:28 /home\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 15:13 /kaggle\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:10 /lib\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:03 /lib32\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:10 /lib64\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:03 /libx32\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:03 /media\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:03 /mnt\n",
            "drwxr-xr-x   - root root       4096 2026-02-12 14:11 /opt\n",
            "dr-xr-xr-x   - root root          0 2026-02-17 15:12 /proc\n",
            "drwxrwxr-x   - root root       4096 2026-01-16 14:11 /python-apt\n",
            "-r-xr-xr-x   1 root root     346012 2000-01-01 08:00 /python-apt.tar.xz\n",
            "drwx------   - root root       4096 2026-02-17 16:31 /root\n",
            "drwxr-xr-x   - root root       4096 2026-01-16 14:07 /run\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 15:13 /sbin\n",
            "drwxr-xr-x   - root root       4096 2026-01-09 02:03 /srv\n",
            "dr-xr-xr-x   - root root          0 2026-02-17 15:12 /sys\n",
            "drwxrwxrwt   - root root       4096 2026-02-17 16:36 /tmp\n",
            "drwxr-xr-x   - root root       4096 2026-02-06 14:15 /tools\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 16:41 /user\n",
            "drwxr-xr-x   - root root       4096 2026-02-11 14:11 /usr\n",
            "drwxr-xr-x   - root root       4096 2026-02-06 14:16 /var\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el archivo today.txt en HDFS.\n",
        "!date > today.txt\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put today.txt /user/root/\n",
        "# Verificar la carga\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /user/root/"
      ],
      "metadata": {
        "id": "AyjggihUghvA",
        "outputId": "506545f3-4acb-4649-bad0-132454d60e7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `/user/root/today.txt': File exists\n",
            "Found 2 items\n",
            "drwxr-xr-x   - root root       4096 2026-02-17 16:41 /user/root/files\n",
            "-rw-r--r--   1 root root         32 2026-02-17 16:51 /user/root/today.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar//leer archivos\n",
        "!cat hoy.txt\n",
        "# Mostrar contenido en HDFS\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cat /user/root/today.txt"
      ],
      "metadata": {
        "id": "-xjHTfdxiK_F",
        "outputId": "a668c702-16c0-4cf9-cffe-85f91d7e7765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 17 04:43:10 PM UTC 2026\n",
            "root\n",
            "Tue Feb 17 04:51:48 PM UTC 2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiar el archivo hoy.txt del origen al directorio files.\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -cp /user/root/today.txt /user/root/files/\n",
        "# Verificacion\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /user/root/files/"
      ],
      "metadata": {
        "id": "T6tFtPKpilP4",
        "outputId": "7f25d8f0-5a46-4bf3-8e08-d032cf7f17cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root root         32 2026-02-17 17:01 /user/root/files/today.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiar el archivo jps.txt desde/hacia el sistema de archivos local a HDFS\n",
        "!jps > jps.txt\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -copyFromLocal jps.txt /user/root/\n",
        "# Copiar DESDE HDFS HACIA el sistema local\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -copyToLocal /user/root/jps.txt jps_descargado.txt"
      ],
      "metadata": {
        "id": "L6PHfsO-jQpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mover el archivo jps.txt de la fuente a files.\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -mv /user/root/jps.txt /user/root/files/\n",
        "# Verificacion\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -ls /user/root/files/\n",
        "# Eliminar el archivo today.txt del directorio principal en HDFS.\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -rm /user/root/today.txt\n",
        "# Mostrar las últimas líneas de jps.txt.\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -tail /user/root/files/jps.txt"
      ],
      "metadata": {
        "id": "jnftTzNsnFSx",
        "outputId": "e1d83c5c-ad6b-4986-c4a6-28b6cc61d69e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root         10 2026-02-17 17:17 /user/root/files/jps.txt\n",
            "-rw-r--r--   1 root root         32 2026-02-17 17:01 /user/root/files/today.txt\n",
            "2026-02-17 17:20:29,056 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "Deleted /user/root/today.txt\n",
            "31703 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2\n",
        "En el siguiente enlace tenéis un ejemplo de uso de otra de las funciones deiponibles en la distribución Hadoop, en este caso un contador de palabras.\n",
        "\n",
        "[Contar palabras con mapReduce](https://www.youtube.com/watch?v=woUzV_liwto)\n",
        "\n",
        "A partir de lo que has visto en este cuaderno, reproduce el ejempllo en tu entorno, ofreciendo una salida en la que por ejemplo se ordenen las palabras de mayo a menor número de apariciones.\n",
        "\n",
        "[El Quijote texto plano](https://gutenberg.org/files/2000/2000-0.txt)\n",
        "\n",
        "Introduce celdas de texto para explicar los pasos que vas realizando."
      ],
      "metadata": {
        "id": "Dj2ya7U0zxRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el libro desde el Proyecto Gutenberg\n",
        "!wget https://www.gutenberg.org/cache/epub/2000/pg2000.txt -O el_quijote.txt\n",
        "\n",
        "# Creamos una carpeta de entrada en HDFS y subimos el archivo\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -mkdir -p /user/root/input_quijote\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -put el_quijote.txt /user/root/input_quijote/"
      ],
      "metadata": {
        "id": "uW2BpiWcMv7q",
        "outputId": "75eed984-6de0-4c27-e940-25b394bc7b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-17 17:29:01--  https://www.gutenberg.org/cache/epub/2000/pg2000.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2225845 (2.1M) [text/plain]\n",
            "Saving to: ‘el_quijote.txt’\n",
            "\n",
            "el_quijote.txt      100%[===================>]   2.12M  1.66MB/s    in 1.3s    \n",
            "\n",
            "2026-02-17 17:29:04 (1.66 MB/s) - ‘el_quijote.txt’ saved [2225845/2225845]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos el job de MapReduce\n",
        "# Parámetros: wordcount <ruta_entrada> <ruta_salida>\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -rm -r /user/root/output_quijote # Borramos salida previa si existe\n",
        "!/usr/local/hadoop-3.3.4/bin/hadoop jar /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar wordcount /user/root/input_quijote /user/root/output_quijote"
      ],
      "metadata": {
        "id": "ZIoPR0jCpYF7",
        "outputId": "09b8d39f-0863-4eba-90cd-c11092ee30b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: `/user/root/output_quijote': No such file or directory\n",
            "2026-02-17 17:29:15,477 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2026-02-17 17:29:15,657 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2026-02-17 17:29:15,657 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2026-02-17 17:29:15,852 INFO input.FileInputFormat: Total input files to process : 1\n",
            "2026-02-17 17:29:15,895 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2026-02-17 17:29:16,254 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1236664096_0001\n",
            "2026-02-17 17:29:16,254 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2026-02-17 17:29:16,524 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2026-02-17 17:29:16,526 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2026-02-17 17:29:16,527 INFO mapreduce.Job: Running job: job_local1236664096_0001\n",
            "2026-02-17 17:29:16,541 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 17:29:16,541 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 17:29:16,543 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
            "2026-02-17 17:29:16,596 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2026-02-17 17:29:16,597 INFO mapred.LocalJobRunner: Starting task: attempt_local1236664096_0001_m_000000_0\n",
            "2026-02-17 17:29:16,637 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 17:29:16,638 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 17:29:16,668 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 17:29:16,676 INFO mapred.MapTask: Processing split: file:/user/root/input_quijote/el_quijote.txt:0+2225845\n",
            "2026-02-17 17:29:16,810 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2026-02-17 17:29:16,810 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2026-02-17 17:29:16,810 INFO mapred.MapTask: soft limit at 83886080\n",
            "2026-02-17 17:29:16,810 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2026-02-17 17:29:16,810 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2026-02-17 17:29:16,815 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2026-02-17 17:29:16,827 INFO input.LineRecordReader: Found UTF-8 BOM and skipped it\n",
            "2026-02-17 17:29:17,532 INFO mapred.LocalJobRunner: \n",
            "2026-02-17 17:29:17,532 INFO mapred.MapTask: Starting flush of map output\n",
            "2026-02-17 17:29:17,532 INFO mapred.MapTask: Spilling map output\n",
            "2026-02-17 17:29:17,532 INFO mapred.MapTask: bufstart = 0; bufend = 3740299; bufvoid = 104857600\n",
            "2026-02-17 17:29:17,532 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24655796(98623184); length = 1558601/6553600\n",
            "2026-02-17 17:29:17,538 INFO mapreduce.Job: Job job_local1236664096_0001 running in uber mode : false\n",
            "2026-02-17 17:29:17,539 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2026-02-17 17:29:18,626 INFO mapred.MapTask: Finished spill 0\n",
            "2026-02-17 17:29:18,661 INFO mapred.Task: Task:attempt_local1236664096_0001_m_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 17:29:18,669 INFO mapred.LocalJobRunner: map\n",
            "2026-02-17 17:29:18,669 INFO mapred.Task: Task 'attempt_local1236664096_0001_m_000000_0' done.\n",
            "2026-02-17 17:29:18,679 INFO mapred.Task: Final Counters for attempt_local1236664096_0001_m_000000_0: Counters: 18\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2524405\n",
            "\t\tFILE: Number of bytes written=1520557\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=38055\n",
            "\t\tMap output records=389651\n",
            "\t\tMap output bytes=3740299\n",
            "\t\tMap output materialized bytes=601948\n",
            "\t\tInput split bytes=109\n",
            "\t\tCombine input records=389651\n",
            "\t\tCombine output records=39753\n",
            "\t\tSpilled Records=39753\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=31\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2243249\n",
            "2026-02-17 17:29:18,679 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236664096_0001_m_000000_0\n",
            "2026-02-17 17:29:18,681 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2026-02-17 17:29:18,688 INFO mapred.LocalJobRunner: Starting task: attempt_local1236664096_0001_r_000000_0\n",
            "2026-02-17 17:29:18,688 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2026-02-17 17:29:18,705 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2026-02-17 17:29:18,707 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2026-02-17 17:29:18,707 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2026-02-17 17:29:18,715 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3dcacab5\n",
            "2026-02-17 17:29:18,717 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2026-02-17 17:29:18,756 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2381106432, maxSingleShuffleLimit=595276608, mergeThreshold=1571530368, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2026-02-17 17:29:18,766 INFO reduce.EventFetcher: attempt_local1236664096_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2026-02-17 17:29:18,831 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1236664096_0001_m_000000_0 decomp: 601944 len: 601948 to MEMORY\n",
            "2026-02-17 17:29:18,836 INFO reduce.InMemoryMapOutput: Read 601944 bytes from map-output for attempt_local1236664096_0001_m_000000_0\n",
            "2026-02-17 17:29:18,838 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 601944, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->601944\n",
            "2026-02-17 17:29:18,839 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2026-02-17 17:29:18,840 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 17:29:18,840 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2026-02-17 17:29:18,850 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-17 17:29:18,850 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 601937 bytes\n",
            "2026-02-17 17:29:18,980 INFO reduce.MergeManagerImpl: Merged 1 segments, 601944 bytes to disk to satisfy reduce memory limit\n",
            "2026-02-17 17:29:18,981 INFO reduce.MergeManagerImpl: Merging 1 files, 601948 bytes from disk\n",
            "2026-02-17 17:29:18,982 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2026-02-17 17:29:18,982 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2026-02-17 17:29:18,983 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 601937 bytes\n",
            "2026-02-17 17:29:18,984 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 17:29:18,987 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2026-02-17 17:29:19,548 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2026-02-17 17:29:19,630 INFO mapred.Task: Task:attempt_local1236664096_0001_r_000000_0 is done. And is in the process of committing\n",
            "2026-02-17 17:29:19,633 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2026-02-17 17:29:19,635 INFO mapred.Task: Task attempt_local1236664096_0001_r_000000_0 is allowed to commit now\n",
            "2026-02-17 17:29:19,640 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1236664096_0001_r_000000_0' to file:/user/root/output_quijote\n",
            "2026-02-17 17:29:19,641 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2026-02-17 17:29:19,644 INFO mapred.Task: Task 'attempt_local1236664096_0001_r_000000_0' done.\n",
            "2026-02-17 17:29:19,644 INFO mapred.Task: Final Counters for attempt_local1236664096_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3728333\n",
            "\t\tFILE: Number of bytes written=2572572\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=39753\n",
            "\t\tReduce shuffle bytes=601948\n",
            "\t\tReduce input records=39753\n",
            "\t\tReduce output records=39753\n",
            "\t\tSpilled Records=39753\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=262144000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=450067\n",
            "2026-02-17 17:29:19,645 INFO mapred.LocalJobRunner: Finishing task: attempt_local1236664096_0001_r_000000_0\n",
            "2026-02-17 17:29:19,647 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2026-02-17 17:29:20,551 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2026-02-17 17:29:20,552 INFO mapreduce.Job: Job job_local1236664096_0001 completed successfully\n",
            "2026-02-17 17:29:20,574 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=6252738\n",
            "\t\tFILE: Number of bytes written=4093129\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=38055\n",
            "\t\tMap output records=389651\n",
            "\t\tMap output bytes=3740299\n",
            "\t\tMap output materialized bytes=601948\n",
            "\t\tInput split bytes=109\n",
            "\t\tCombine input records=389651\n",
            "\t\tCombine output records=39753\n",
            "\t\tReduce input groups=39753\n",
            "\t\tReduce shuffle bytes=601948\n",
            "\t\tReduce input records=39753\n",
            "\t\tReduce output records=39753\n",
            "\t\tSpilled Records=79506\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=31\n",
            "\t\tTotal committed heap usage (bytes)=524288000\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=2243249\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=450067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traemos el resultado de HDFS a la máquina local (Colab)\n",
        "!/usr/local/hadoop-3.3.4/bin/hdfs dfs -get /user/root/output_quijote/part-r-00000 resultado_quijote.txt\n",
        "\n",
        "# Ordenamos el resultado:\n",
        "# -k2,2n: Ordenar por la segunda columna (el conteo) de forma numérica\n",
        "# -r: De forma reversa (mayor a menor)\n",
        "# head -n 20: Mostrar las 20 palabras más frecuentes\n",
        "!sort -k2,2nr resultado_quijote.txt | head -n 20"
      ],
      "metadata": {
        "id": "Y9FqK7LdpbLs",
        "outputId": "b49ec889-16d0-48e9-9d7e-ca82661375f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "que\t19546\n",
            "de\t18133\n",
            "y\t15976\n",
            "la\t10329\n",
            "a\t9627\n",
            "el\t8009\n",
            "en\t7941\n",
            "no\t5622\n",
            "se\t4751\n",
            "los\t4701\n",
            "con\t4119\n",
            "por\t3763\n",
            "las\t3440\n",
            "lo\t3418\n",
            "le\t3405\n",
            "su\t3355\n",
            "—\t2983\n",
            "don\t2597\n",
            "del\t2501\n",
            "me\t2344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "/usr/local/hadoop-3.3.4/bin/hadoop jar \\\n",
        "  /usr/local/hadoop-3.3.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.4.jar \\\n",
        "  wordcount ~/input ~/wc_example"
      ],
      "metadata": {
        "id": "VmEePLSiMwdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/wc_example/*"
      ],
      "metadata": {
        "id": "cojFPFLBM7-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}